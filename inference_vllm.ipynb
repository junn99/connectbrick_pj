{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: psutil in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (6.1.0)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting numpy<2.0.0 (from vllm)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (4.67.0)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.45.2 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (4.47.0.dev0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (0.20.3)\n",
      "Requirement already satisfied: protobuf in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (5.28.3)\n",
      "Requirement already satisfied: aiohttp in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (3.11.7)\n",
      "Collecting openai>=1.45.0 (from vllm)\n",
      "  Downloading openai-1.55.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn[standard] (from vllm)\n",
      "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pydantic>=2.9 (from vllm)\n",
      "  Downloading pydantic-2.10.0-py3-none-any.whl.metadata (167 kB)\n",
      "Collecting pillow (from vllm)\n",
      "  Using cached pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting prometheus-client>=0.18.0 (from vllm)\n",
      "  Using cached prometheus_client-0.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm)\n",
      "  Using cached lm_format_enforcer-0.10.9-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines<0.1,>=0.0.43 (from vllm)\n",
      "  Using cached outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.10.4 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (3.16.1)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Using cached partial_json_parser-0.2.1.1.post4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyzmq in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (26.2.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm)\n",
      "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (8.5.0)\n",
      "Collecting mistral-common>=1.5.0 (from mistral-common[opencv]>=1.5.0->vllm)\n",
      "  Downloading mistral_common-1.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (6.0.2)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting compressed-tensors==0.8.0 (from vllm)\n",
      "  Using cached compressed_tensors-0.8.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting ray>=2.9 (from vllm)\n",
      "  Downloading ray-2.39.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting nvidia-ml-py>=12.560.30 (from vllm)\n",
      "  Using cached nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from vllm) (2.5.1)\n",
      "Collecting torchvision==0.20.1 (from vllm)\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.28.post3 (from vllm)\n",
      "  Using cached xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from vllm)\n",
      "  Using cached fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: networkx in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from torch==2.5.1->vllm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1->vllm) (1.3.0)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer<0.11,>=0.10.9->vllm)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (24.2)\n",
      "Collecting jsonschema<5.0.0,>=4.21.1 (from mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pillow (from vllm)\n",
      "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting opencv-python-headless<5.0.0,>=4.0.0 (from mistral-common[opencv]>=1.5.0->vllm)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=1.45.0->vllm)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.45.0->vllm)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.45.0->vllm)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.45.0->vllm)\n",
      "  Downloading jiter-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.45.0->vllm)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting lark (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (1.6.0)\n",
      "Collecting cloudpickle (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting numba (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting referencing (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: datasets in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (3.1.0)\n",
      "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Using cached pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->vllm)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.0 (from pydantic>=2.9->vllm)\n",
      "  Downloading pydantic_core-2.27.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from ray>=2.9->vllm) (8.1.7)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.9->vllm)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: aiosignal in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from tokenizers>=0.19.1->vllm) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from transformers>=4.45.2->vllm) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from aiohttp->vllm) (2.4.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from aiohttp->vllm) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from aiohttp->vllm) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from aiohttp->vllm) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from aiohttp->vllm) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from aiohttp->vllm) (1.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from importlib-metadata->vllm) (3.21.0)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->vllm)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]->vllm)\n",
      "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.45.0->vllm) (1.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.45.0->vllm)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm)\n",
      "  Downloading rpds_py-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from jinja2->torch==2.5.1->vllm) (3.0.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm) (1.16.0)\n",
      "Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl (198.9 MB)\n",
      "Using cached compressed_tensors-0.8.0-py3-none-any.whl (86 kB)\n",
      "Using cached gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "Using cached fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
      "Using cached lm_format_enforcer-0.10.9-py3-none-any.whl (43 kB)\n",
      "Downloading mistral_common-1.5.1-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m170.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "Downloading openai-1.55.0-py3-none-any.whl (389 kB)\n",
      "Using cached outlines-0.0.46-py3-none-any.whl (101 kB)\n",
      "Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached prometheus_client-0.21.0-py3-none-any.whl (54 kB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic-2.10.0-py3-none-any.whl (454 kB)\n",
      "Downloading pydantic_core-2.27.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.39.0-cp310-cp310-manylinux2014_x86_64.whl (66.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Downloading msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post4-py3-none-any.whl (9.9 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m155.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m148.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m224.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rpds_py-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
      "Installing collected packages: sentencepiece, pyairports, py-cpuinfo, nvidia-ml-py, websockets, uvloop, sniffio, rpds-py, python-dotenv, pydantic-core, pycountry, prometheus-client, pillow, partial-json-parser, numpy, msgspec, msgpack, llvmlite, lark, jiter, interegular, httptools, h11, einops, distro, diskcache, cloudpickle, annotated-types, uvicorn, tiktoken, referencing, pydantic, opencv-python-headless, numba, httpcore, gguf, anyio, watchfiles, starlette, lm-format-enforcer, jsonschema-specifications, httpx, xformers, torchvision, prometheus-fastapi-instrumentator, openai, jsonschema, fastapi, ray, mistral-common, compressed-tensors, outlines, vllm\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 cloudpickle-3.1.0 compressed-tensors-0.8.0 diskcache-5.6.3 distro-1.9.0 einops-0.8.0 fastapi-0.115.5 gguf-0.10.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.27.2 interegular-0.3.3 jiter-0.7.1 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 lark-1.2.2 llvmlite-0.43.0 lm-format-enforcer-0.10.9 mistral-common-1.5.1 msgpack-1.1.0 msgspec-0.18.6 numba-0.60.0 numpy-1.26.4 nvidia-ml-py-12.560.30 openai-1.55.0 opencv-python-headless-4.10.0.84 outlines-0.0.46 partial-json-parser-0.2.1.1.post4 pillow-10.4.0 prometheus-client-0.21.0 prometheus-fastapi-instrumentator-7.0.0 py-cpuinfo-9.0.0 pyairports-2.1.1 pycountry-24.6.1 pydantic-2.10.0 pydantic-core-2.27.0 python-dotenv-1.0.1 ray-2.39.0 referencing-0.35.1 rpds-py-0.21.0 sentencepiece-0.2.0 sniffio-1.3.1 starlette-0.41.3 tiktoken-0.7.0 torchvision-0.20.1 uvicorn-0.32.1 uvloop-0.21.0 vllm-0.6.4.post1 watchfiles-0.24.0 websockets-14.1 xformers-0.0.28.post3\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eardream2/miniconda3/envs/single/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-21 19:17:00,659\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List\n",
    "from vllm.outputs import RequestOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model= 'junn991/llama3-8b-1gpu-couple' # SFT모델이 여기에 들어가야함 => 저장된 SFT 절대경로로 지정\n",
    "gpu_num= 1\n",
    "max_token: int = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-21 19:17:38 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 11-21 19:17:38 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='junn991/llama3-8b-1gpu-couple', speculative_config=None, tokenizer='junn991/llama3-8b-1gpu-couple', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=junn991/llama3-8b-1gpu-couple, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 11-21 19:17:40 selector.py:135] Using Flash Attention backend.\n",
      "INFO 11-21 19:17:41 model_runner.py:1072] Starting to load model junn991/llama3-8b-1gpu-couple...\n",
      "INFO 11-21 19:17:41 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.20s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.49s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:15<00:04,  4.88s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  7.30s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  6.64s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-21 19:20:10 model_runner.py:1077] Loading model weights took 14.9595 GB\n",
      "INFO 11-21 19:20:11 worker.py:232] Memory profiling results: total_gpu_memory=21.96GiB initial_memory_usage=15.27GiB peak_torch_memory=16.15GiB memory_usage_post_profile=15.30GiB non_torch_memory=0.34GiB kv_cache_size=1.09GiB gpu_memory_utilization=0.80\n",
      "INFO 11-21 19:20:12 gpu_executor.py:113] # GPU blocks: 557, # CPU blocks: 2048\n",
      "INFO 11-21 19:20:12 gpu_executor.py:117] Maximum concurrency for 2048 tokens per request: 4.35x\n",
      "INFO 11-21 19:20:14 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-21 19:20:14 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-21 19:20:36 model_runner.py:1518] Graph capturing finished in 22 secs, took 0.82 GiB\n"
     ]
    }
   ],
   "source": [
    "model = LLM(model=base_model, tensor_parallel_size=gpu_num, max_model_len=max_token, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(top_k=5, top_p=1, max_tokens=max_token, temperature=0.5, best_of=13, n=6,\n",
    "                                     stop=['<|endoftext|>', '</s>', '<|im_end|>','<|end_of_text|>'])\n",
    "# 답변 정도를 보여주는 걸로 n=6으로 설정ㅇㅇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts= ['점심 먹었어?', '뭐해!?','너무 졸려 근데 일이 많아...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/39 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-21 19:51:00 scheduler.py:1481] Sequence group 5_parallel_sample_6 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|▊         | 3/39 [02:35<31:09, 51.92s/it, est. speed input: 0.15 toks/s, output: 42.93 toks/s]   \n"
     ]
    }
   ],
   "source": [
    "outputs: List[RequestOutput] = model.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'점심 먹었어?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사랑해'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너무 졸려 근데 일이 많아...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=0, text='?\\n아까 먹었지~\\n뭐 먹었어??\\n닭가슴살 먹었어~\\n오 맛있었겠다~~\\n응~ 닭가슴살에 양파랑 먹었어~', token_ids=(5380, 54059, 101154, 108715, 101461, 22035, 89241, 167, 99834, 108715, 101461, 32179, 30, 5380, 9019, 255, 20565, 120534, 108281, 108715, 101461, 32179, 89241, 58368, 119408, 105625, 101461, 125684, 5940, 198, 110685, 93, 35243, 255, 20565, 120534, 108281, 19954, 104870, 101508, 102581, 108715, 101461, 32179, 93, 128001), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=<|end_of_text|>),\n",
       " CompletionOutput(index=1, text='?\\n응응 ㅎㅎ\\n오늘도 비와서\\n오늘은\\n어디로 가는거야?\\n오늘은\\n#@이름', token_ids=(5380, 110685, 110685, 103667, 236, 116507, 198, 58368, 105622, 49085, 75086, 81673, 27796, 198, 58368, 105622, 34804, 198, 32179, 90335, 17835, 36609, 16969, 93292, 90759, 5380, 58368, 105622, 34804, 198, 83172, 13094, 64254, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=2),\n",
       " CompletionOutput(index=2, text='?\\n아니\\n배고파\\n나도..\\n배고파\\n나도\\n배고파\\n나도\\n나도\\n나도', token_ids=(5380, 54059, 84136, 198, 103588, 35495, 101508, 198, 61415, 49085, 35047, 103588, 35495, 101508, 198, 61415, 49085, 198, 103588, 35495, 101508, 198, 61415, 49085, 198, 61415, 49085, 198, 61415, 49085, 128001), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=<|end_of_text|>),\n",
       " CompletionOutput(index=3, text='?\\n응응ㅋㅋㅋ\\n오늘은 뭐먹었어??\\n오늘은 닭가슴살이랑\\n샐러드 먹었어ㅋㅋㅋ\\n아침엔 오렌지 먹었고ㅋㅋ\\n오렌지 맛있었겠다\\n나는 아침에 바나나 먹었어ㅋㅋㅋ\\n바나나도 맛있지ㅋㅋㅋ\\n응응ㅋㅋ\\n오랜만에 먹어서 맛있었어ㅋㅋㅋ', token_ids=(5380, 110685, 110685, 102576, 101900, 198, 58368, 105622, 34804, 113792, 115595, 101461, 32179, 30, 5380, 58368, 105622, 34804, 35243, 255, 20565, 120534, 108281, 13094, 102581, 198, 70482, 238, 61394, 30446, 108715, 101461, 32179, 102576, 101900, 198, 54059, 108308, 108733, 74177, 111932, 22035, 108715, 101461, 35495, 102576, 198, 58368, 111932, 22035, 119408, 105625, 101461, 125684, 198, 110955, 126474, 19954, 82818, 61415, 61415, 108715, 101461, 32179, 102576, 101900, 198, 101974, 61415, 61415, 49085, 119408, 105625, 22035, 102576, 101900, 198, 110685, 110685, 102576, 198, 58368, 105501, 73653, 19954, 108715, 108503, 119408, 105625, 101461, 32179, 102576, 101900, 128001), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=<|end_of_text|>),\n",
       " CompletionOutput(index=4, text='?\\n응응 먹었어\\n뭐먹었어??\\n아침에 먹은거랑 똑같이 먹었어\\n아하 그렇구나\\n나는 점심 먹었어\\n뭐먹었어??\\n나는 오늘은 간단하게 먹었어', token_ids=(5380, 110685, 110685, 108715, 101461, 32179, 198, 167, 99834, 115595, 101461, 32179, 30, 5380, 54059, 108308, 19954, 108715, 34804, 93292, 102581, 125825, 239, 111843, 13094, 108715, 101461, 32179, 198, 54059, 16582, 119879, 89359, 61415, 198, 110955, 106313, 102612, 108715, 101461, 32179, 198, 167, 99834, 115595, 101461, 32179, 30, 5380, 110955, 111128, 34804, 105131, 101353, 102893, 108715, 101461, 32179, 128001), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=<|end_of_text|>),\n",
       " CompletionOutput(index=5, text='?\\n응응 먹었엉\\n뭐먹었어??\\n나도 먹을거얌\\n나 떡볶이 먹었어\\n떡볶이 맛있었겠다\\n응응 맛있었엉\\n나도 떡볶이 먹을까', token_ids=(5380, 110685, 110685, 108715, 101461, 13879, 231, 198, 167, 99834, 115595, 101461, 32179, 30, 5380, 61415, 49085, 108715, 18359, 93292, 28498, 234, 198, 61415, 105136, 94, 29099, 114, 13094, 108715, 101461, 32179, 198, 104207, 94, 29099, 114, 13094, 119408, 105625, 101461, 125684, 198, 110685, 110685, 119408, 105625, 101461, 13879, 231, 198, 61415, 49085, 105136, 94, 29099, 114, 13094, 108715, 18359, 101154, 128001), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=<|end_of_text|>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n아까 먹었지~\\n뭐 먹었어??\\n닭가슴살 먹었어~\\n오 맛있었겠다~~\\n응~ 닭가슴살에 양파랑 먹었어~'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웅웅!!\n"
     ]
    }
   ],
   "source": [
    "def extract_text_between_newlines(text):\n",
    "    \"\"\"\n",
    "    주어진 문자열에서 첫 번째 \\n과 두 번째 \\n 사이의 텍스트를 추출합니다.\n",
    "    \\n이 없을 경우 전체 텍스트를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 입력 문자열.\n",
    "\n",
    "    Returns:\n",
    "        str: 첫 번째와 두 번째 \\n 사이의 텍스트, 또는 \\n이 없으면 입력 텍스트 그대로 반환.\n",
    "    \"\"\"\n",
    "    # 첫 번째 \\n 위치 찾기\n",
    "    first_newline = text.find('\\n')\n",
    "    \n",
    "    # \\n이 없다면 입력 텍스트 그대로 반환\n",
    "    if first_newline == -1:\n",
    "        return text.strip()\n",
    "    \n",
    "    # 두 번째 \\n 위치 찾기\n",
    "    second_newline = text.find('\\n', first_newline + 1)\n",
    "    \n",
    "    # 두 번째 \\n이 존재하면 텍스트 추출, 없으면 첫 번째 \\n 이후 텍스트 반환\n",
    "    if second_newline != -1:\n",
    "        return text[first_newline + 1:second_newline].strip()\n",
    "    else:\n",
    "        return text[first_newline + 1:].strip()\n",
    "\n",
    "# 테스트\n",
    "result = extract_text_between_newlines(outputs[0].outputs[0].text)\n",
    "print(result)  # \"아까 먹었지~\" 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================(0, 0)번째 답변====================================\n",
      "웅웅!!\n",
      "===================(0, 1)번째 답변====================================\n",
      "응응 먹었어\n",
      "===================(0, 2)번째 답변====================================\n",
      "아니용\n",
      "===================(0, 3)번째 답변====================================\n",
      "응응\n",
      "===================(0, 4)번째 답변====================================\n",
      "웅웅\n",
      "===================(0, 5)번째 답변====================================\n",
      "아니 아직 ㅎㅎ\n",
      "===================(1, 0)번째 답변====================================\n",
      "아까 밥먹구\n",
      "===================(1, 1)번째 답변====================================\n",
      "나\n",
      "===================(1, 2)번째 답변====================================\n",
      "나 밥먹었어 ㅎㅎ\n",
      "===================(1, 3)번째 답변====================================\n",
      "나 지금\n",
      "===================(1, 4)번째 답변====================================\n",
      "나\n",
      "===================(1, 5)번째 답변====================================\n",
      "ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
      "===================(2, 0)번째 답변====================================\n",
      "ㅠㅠㅠ\n",
      "===================(2, 1)번째 답변====================================\n",
      "ㅠㅠㅠㅠㅠ\n",
      "===================(2, 2)번째 답변====================================\n",
      "ㅠㅠㅠㅠㅠ\n",
      "===================(2, 3)번째 답변====================================\n",
      "ㅠㅠㅠㅠㅠ\n",
      "===================(2, 4)번째 답변====================================\n",
      "#@이름\n",
      "===================(2, 5)번째 답변====================================\n",
      "ㅠㅠㅠㅠㅠㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # print(f'==================={i}번째 답변====================================')\n",
    "    # print(extract_text_between_newlines(outputs[i].outputs[0].text))\n",
    "    for j in range(6):\n",
    "            print(f'==================={i,j}번째 답변====================================')\n",
    "            print(extract_text_between_newlines(outputs[i].outputs[j].text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "single",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
