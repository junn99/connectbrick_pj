import streamlit as st
from vllm import LLM, SamplingParams
import time

st.title("MZ Talk ChatBot")

# Sidebar configurations
with st.sidebar:
    clear_button = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    
    # ê³ ì •ëœ ëª¨ë¸ ì‚¬ìš©
    model = "junn991/mz_talk"
    st.info(f'ì‚¬ìš© ëª¨ë¸: {model}')
    
    # vLLM specific configurations
    num_gpus = st.slider("GPU ìˆ˜", min_value=1, max_value=8, value=1)
    temperature = st.slider("Temperature(ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ë‹µë³€)", min_value=0.1, max_value=1.0, value=0.7, step=0.1)
    max_tokens = st.slider("Max Tokens(ì¶œë ¥ ê¸¸ì´)", min_value=64, max_value=2048, value=512)
    
    st.divider()
    st.caption("Made with vLLM ğŸš€")

# Initialize session state
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "llm" not in st.session_state:
    st.session_state["llm"] = None

def initialize_vllm():
    """Initialize vLLM model with current settings"""
    if st.session_state["llm"] is None:
        with st.spinner("MZ Talk ëª¨ë¸ì„ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤... â³"):
            try:
                sampling_params = SamplingParams(
                    temperature=temperature,
                    top_p=0.95,
                    max_tokens=max_tokens,
                    stop=['<|endoftext|>', '</s>', '<|im_end|>', '<|end_of_text|>']
                )
                
                llm = LLM(
                    model=model,
                    tensor_parallel_size=num_gpus,
                    trust_remote_code=True,
                    max_model_len=max_tokens,
                    gpu_memory_utilization=0.8  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì„¤ì •
                )
                
                st.session_state["llm"] = (llm, sampling_params)
                st.success("ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì±„íŒ…ì„ ì‹œì‘í•´ë³´ì„¸ìš” ğŸ˜Š")
            except Exception as e:
                st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def generate_response(prompt):
    """Generate response using vLLM"""
    llm, sampling_params = st.session_state["llm"]
    try:
        outputs = llm.generate([prompt], sampling_params)
        return outputs[0].outputs[0].text.strip()
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def display_messages():
    """Display all messages in the chat"""
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"], avatar="ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"):
            st.write(message["content"])

# Clear chat history if button is clicked
if clear_button:
    st.session_state["messages"] = []
    st.success("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")

# Display chat history
display_messages()

# Initialize vLLM if not already initialized
if st.session_state["llm"] is None:
    initialize_vllm()

# Chat input
if user_input := st.chat_input("MZ Talkê³¼ ëŒ€í™”í•´ë³´ì„¸ìš”..."):
    # Display user message
    st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").write(user_input)
    st.session_state["messages"].append({"role": "user", "content": user_input})
    
    # Generate and display response
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        if st.session_state["llm"] is not None:
            with st.spinner("ìƒê°í•˜ëŠ” ì¤‘..."):
                response = generate_response(user_input)
                st.write(response)
                st.session_state["messages"].append({"role": "assistant", "content": response})
        else:
            st.error("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")